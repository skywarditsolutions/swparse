{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'https://www.youtube.com/watch?v=wofB1wzyYYI', 'url': 'https://www.youtube.com/watch?v=wofB1wzyYYI'}\n",
      "{'text': 'http://github.com/VikParuchuri/marker/blob/master/tests/renderers/test_markdown_renderer.py', 'url': 'http://github.com/VikParuchuri/marker/blob/master/tests/renderers/test_markdown_renderer.py'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "line = \"\"\"\n",
    "https://www.youtube.com/watch?v=wofB1wzyYYI\n",
    "http://github.com/VikParuchuri/marker/blob/master/tests/renderers/test_markdown_renderer.py\n",
    "This is a valid text line without a link.\n",
    "\"\"\"\n",
    " \n",
    "url_pattern = re.compile(r\"http?s?://[^\\s]+\")\n",
    "\n",
    " \n",
    "links = []\n",
    "matches =url_pattern.findall(line)\n",
    "for match in matches:\n",
    " \n",
    "    links.append({\n",
    "        \"text\": match,\n",
    "        \"url\": match\n",
    "    })\n",
    "\n",
    "for link in links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from lxml import html\n",
    "\n",
    "def extract_tables_from_html(html_content: str) -> list[DataFrame] | None:\n",
    "    tree = html.fromstring(html_content)\n",
    "    tables = tree.xpath('//table')\n",
    "\n",
    "    if not tables:\n",
    "        return None\n",
    "\n",
    "    tables_html = [html.tostring(table, method=\"html\").decode() for table in tables]\n",
    "\n",
    "    print(tables_html)\n",
    "    dfs:list[DataFrame] = []\n",
    "    for table_html in tables_html:\n",
    "        try:\n",
    "            df = pd.read_html(table_html)[0]\n",
    "            dfs.append(df)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return dfs if dfs else None\n",
    " \n",
    "def save_tables_to_markdown(dfs: list[DataFrame], file_name: str) -> None:\n",
    "    with open(file_name, 'w') as md_file:\n",
    "        for i, df in enumerate(dfs):\n",
    "            md_file.write(f\"## Table {i + 1}\\n\\n\")\n",
    "            md_file.write(df.to_markdown())\n",
    "            md_file.write(\"\\n\\n\") \n",
    " \n",
    "with open('sample.html', 'r') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "dfs = extract_tables_from_html(html_content)\n",
    "\n",
    "if dfs is not None:\n",
    "    save_tables_to_markdown(dfs, 'tables.md')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "\n",
    "def extract_tables_from_html(html_content: str) -> list[DataFrame] | None:\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if not tables:\n",
    "        print(\"There is no table\")\n",
    "        return None\n",
    "\n",
    "    tables_html = [str(table) for table in tables]\n",
    "\n",
    "    dfs:list[DataFrame] = []\n",
    "    for table_html in tables_html:\n",
    "        try:\n",
    "            str_buffer = io.StringIO(table_html)\n",
    "            df = pd.read_html(str_buffer)[0]\n",
    "            dfs.append(df)\n",
    "        except ValueError:\n",
    "            print(\"There is no table\")\n",
    "            continue\n",
    "    if dfs:\n",
    "        return dfs\n",
    "    return None\n",
    "\n",
    "def save_tables_to_markdown(dfs: list[DataFrame], file_name: str) -> None:\n",
    "    with open(file_name, \"w\") as md_file:\n",
    "        for i, df in enumerate(dfs):\n",
    "            md_file.write(f\"## Table {i + 1}\\n\\n\")\n",
    "            md_file.write(df.to_markdown())\n",
    "            md_file.write(\"\\n\\n\")\n",
    "\n",
    "with open(\"sample.html\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "dfs = extract_tables_from_html(html_content)\n",
    "if dfs is not None:\n",
    "    save_tables_to_markdown(dfs, \"tables.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: image_0.png\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "pxl_doc = openpyxl.load_workbook('DimOrder.xlsx')\n",
    "sheet = pxl_doc['ORDER_DETAILS']\n",
    " \n",
    "for i, image in enumerate(sheet._images):\n",
    "  \n",
    "    if isinstance(image, Image):\n",
    "     \n",
    "        img_data = image.ref  \n",
    "    \n",
    "        pil_img = PILImage.open(img_data)\n",
    " \n",
    "        img_filename = f\"image_{i}.png\"  \n",
    "        pil_img.save(img_filename)   \n",
    "        print(f\"Saved image: {img_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Order', 3, 5]\n"
     ]
    }
   ],
   "source": [
    "sheet_index = [\"1\", \"Order\", \"3\", \"5\"]\n",
    "\n",
    "parsed_list = [int(item) if item.isdigit() else item for item in sheet_index]\n",
    "\n",
    "print(parsed_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image from ORDER_DETAILS: ORDER_DETAILS_image_0.png\n",
      "Saved image from customer: customer_image_0.png\n",
      "Saved image from vendor: vendor_image_0.png\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    " \n",
    "pxl_doc = openpyxl.load_workbook('DimOrder.xlsx')\n",
    " \n",
    "for sheet_name in pxl_doc.sheetnames:\n",
    "    sheet = pxl_doc[sheet_name]\n",
    " \n",
    "    for i, image in enumerate(sheet._images):\n",
    "     \n",
    "        if isinstance(image, Image):\n",
    "            img_data = image.ref   \n",
    "         \n",
    "            pil_img = PILImage.open(img_data)\n",
    " \n",
    "            img_filename = f\"{sheet_name}_image_{i}.png\"   \n",
    "            pil_img.save(img_filename)\n",
    "            print(f\"Saved image from {sheet_name}: {img_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "from uuid import uuid4   \n",
    "\n",
    "s3fs = S3FileSystem(\n",
    "    endpoint_url= ENDPOINT_URL,\n",
    "    key=MINIO_ROOT_USER,\n",
    "    secret=MINIO_ROOT_PASSWORD,\n",
    "    use_ssl=False,\n",
    ")\n",
    "\n",
    "def save_file_s3(s3fs: S3FileSystem, file_name: str, content: bytes | str) -> str:\n",
    "    new_uuid = uuid4()\n",
    "    s3_url = f\"{BUCKET_NAME}/{new_uuid}_{file_name}\"\n",
    "    if isinstance(content, str):\n",
    "        content = content.encode(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    with s3fs.open(s3_url, mode=\"wb\") as f:\n",
    "        f.write(content)\n",
    "        return s3_url\n",
    "\n",
    "def save_img_s3(s3fs: S3FileSystem, image_name:str, image:\"Image\") -> str:\n",
    "    image_name = image_name.lower()\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=image_name.split(\".\")[-1])\n",
    "    img_b = buffered.getvalue()\n",
    "    return save_file_s3(s3fs, image_name, img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    "from s3fs import S3FileSystem\n",
    "import io \n",
    "\n",
    "def extract_excel_images(s3fs: S3FileSystem, excel_content: bytes) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract images from an Excel file and store them in S3.\n",
    "    return a dictionary mapping image names to S3 paths.\n",
    "    \"\"\"\n",
    "    pxl_doc = load_workbook(filename=io.BytesIO(excel_content))\n",
    "    all_images = {}\n",
    "    \n",
    "    for sheet_name in pxl_doc.sheetnames:\n",
    "        sheet = pxl_doc[sheet_name]\n",
    "        \n",
    "        for i, image in enumerate(sheet._images):\n",
    "            if isinstance(image, Image):\n",
    "                img_data = image.ref\n",
    "                pil_image = PILImage.open(img_data)\n",
    "\n",
    "                img_name = f\"{sheet_name}_image_{i}.png\"\n",
    "                saved_img_path = save_img_s3(s3fs, img_name, pil_image)\n",
    "                \n",
    "                all_images[img_name] = saved_img_path\n",
    "\n",
    "    return all_images\n",
    "\n",
    "xlsx_filepath = \"DimOrder.xlsx\"\n",
    "\n",
    "\n",
    "with open(xlsx_filepath, mode=\"rb\") as f:\n",
    "    content = f.read()\n",
    "    images = extract_excel_images(s3fs, content)\n",
    "    print(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
