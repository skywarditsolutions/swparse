{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'https://www.youtube.com/watch?v=wofB1wzyYYI', 'url': 'https://www.youtube.com/watch?v=wofB1wzyYYI'}\n",
      "{'text': 'http://github.com/VikParuchuri/marker/blob/master/tests/renderers/test_markdown_renderer.py', 'url': 'http://github.com/VikParuchuri/marker/blob/master/tests/renderers/test_markdown_renderer.py'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "line = \"\"\"\n",
    "https://www.youtube.com/watch?v=wofB1wzyYYI\n",
    "http://github.com/VikParuchuri/marker/blob/master/tests/renderers/test_markdown_renderer.py\n",
    "This is a valid text line without a link.\n",
    "\"\"\"\n",
    " \n",
    "url_pattern = re.compile(r\"http?s?://[^\\s]+\")\n",
    "\n",
    " \n",
    "links = []\n",
    "matches =url_pattern.findall(line)\n",
    "for match in matches:\n",
    " \n",
    "    links.append({\n",
    "        \"text\": match,\n",
    "        \"url\": match\n",
    "    })\n",
    "\n",
    "for link in links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m             md_file\u001b[38;5;241m.\u001b[39mwrite(df\u001b[38;5;241m.\u001b[39mto_markdown())\n\u001b[1;32m     30\u001b[0m             md_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msample.html\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     33\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     35\u001b[0m dfs \u001b[38;5;241m=\u001b[39m extract_tables_from_html(html_content)\n",
      "File \u001b[0;32m~/Desktop/swparse/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample.html'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from lxml import html\n",
    "\n",
    "def extract_tables_from_html(html_content: str) -> list[DataFrame] | None:\n",
    "    tree = html.fromstring(html_content)\n",
    "    tables = tree.xpath('//table')\n",
    "\n",
    "    if not tables:\n",
    "        return None\n",
    "\n",
    "    tables_html = [html.tostring(table, method=\"html\").decode() for table in tables]\n",
    "\n",
    "    print(tables_html)\n",
    "    dfs:list[DataFrame] = []\n",
    "    for table_html in tables_html:\n",
    "        try:\n",
    "            df = pd.read_html(table_html)[0]\n",
    "            dfs.append(df)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return dfs if dfs else None\n",
    " \n",
    "def save_tables_to_markdown(dfs: list[DataFrame], file_name: str) -> None:\n",
    "    with open(file_name, 'w') as md_file:\n",
    "        for i, df in enumerate(dfs):\n",
    "            md_file.write(f\"## Table {i + 1}\\n\\n\")\n",
    "            md_file.write(df.to_markdown())\n",
    "            md_file.write(\"\\n\\n\") \n",
    " \n",
    "with open('sample.html', 'r') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "dfs = extract_tables_from_html(html_content)\n",
    "\n",
    "if dfs is not None:\n",
    "    save_tables_to_markdown(dfs, 'tables.md')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "\n",
    "def extract_tables_from_html(html_content: str) -> list[DataFrame] | None:\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if not tables:\n",
    "        print(\"There is no table\")\n",
    "        return None\n",
    "\n",
    "    tables_html = [str(table) for table in tables]\n",
    "\n",
    "    dfs:list[DataFrame] = []\n",
    "    for table_html in tables_html:\n",
    "        try:\n",
    "            str_buffer = io.StringIO(table_html)\n",
    "            df = pd.read_html(str_buffer)[0]\n",
    "            dfs.append(df)\n",
    "        except ValueError:\n",
    "            print(\"There is no table\")\n",
    "            continue\n",
    "    if dfs:\n",
    "        return dfs\n",
    "    return None\n",
    "\n",
    "def save_tables_to_markdown(dfs: list[DataFrame], file_name: str) -> None:\n",
    "    with open(file_name, \"w\") as md_file:\n",
    "        for i, df in enumerate(dfs):\n",
    "            md_file.write(f\"## Table {i + 1}\\n\\n\")\n",
    "            md_file.write(df.to_markdown())\n",
    "            md_file.write(\"\\n\\n\")\n",
    "\n",
    "with open(\"sample.html\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "dfs = extract_tables_from_html(html_content)\n",
    "if dfs is not None:\n",
    "    save_tables_to_markdown(dfs, \"tables.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: image_0.png\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "pxl_doc = openpyxl.load_workbook('DimOrder.xlsx')\n",
    "sheet = pxl_doc['ORDER_DETAILS']\n",
    " \n",
    "for i, image in enumerate(sheet._images):\n",
    "  \n",
    "    if isinstance(image, Image):\n",
    "     \n",
    "        img_data = image.ref  \n",
    "    \n",
    "        pil_img = PILImage.open(img_data)\n",
    " \n",
    "        img_filename = f\"image_{i}.png\"  \n",
    "        pil_img.save(img_filename)   \n",
    "        print(f\"Saved image: {img_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Order', 3, 5]\n"
     ]
    }
   ],
   "source": [
    "sheet_index = [\"1\", \"Order\", \"3\", \"5\"]\n",
    "\n",
    "parsed_list = [int(item) if item.isdigit() else item for item in sheet_index]\n",
    "\n",
    "print(parsed_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image from ORDER_DETAILS: ORDER_DETAILS_image_0.png\n",
      "Saved image from customer: customer_image_0.png\n",
      "Saved image from vendor: vendor_image_0.png\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    " \n",
    "pxl_doc = openpyxl.load_workbook('DimOrder.xlsx')\n",
    " \n",
    "for sheet_name in pxl_doc.sheetnames:\n",
    "    sheet = pxl_doc[sheet_name]\n",
    " \n",
    "    for i, image in enumerate(sheet._images):\n",
    "     \n",
    "        if isinstance(image, Image):\n",
    "            img_data = image.ref   \n",
    "         \n",
    "            pil_img = PILImage.open(img_data)\n",
    " \n",
    "            img_filename = f\"{sheet_name}_image_{i}.png\"   \n",
    "            pil_img.save(img_filename)\n",
    "            print(f\"Saved image from {sheet_name}: {img_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "from uuid import uuid4   \n",
    "\n",
    "s3fs = S3FileSystem(\n",
    "    endpoint_url= ENDPOINT_URL,\n",
    "    key=MINIO_ROOT_USER,\n",
    "    secret=MINIO_ROOT_PASSWORD,\n",
    "    use_ssl=False,\n",
    ")\n",
    "\n",
    "def save_file_s3(s3fs: S3FileSystem, file_name: str, content: bytes | str) -> str:\n",
    "    new_uuid = uuid4()\n",
    "    s3_url = f\"{BUCKET_NAME}/{new_uuid}_{file_name}\"\n",
    "    if isinstance(content, str):\n",
    "        content = content.encode(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    with s3fs.open(s3_url, mode=\"wb\") as f:\n",
    "        f.write(content)\n",
    "        return s3_url\n",
    "\n",
    "def save_img_s3(s3fs: S3FileSystem, image_name:str, image:\"Image\") -> str:\n",
    "    image_name = image_name.lower()\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=image_name.split(\".\")[-1])\n",
    "    img_b = buffered.getvalue()\n",
    "    return save_file_s3(s3fs, image_name, img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    "from s3fs import S3FileSystem\n",
    "import io \n",
    "\n",
    "def extract_excel_images(s3fs: S3FileSystem, excel_content: bytes) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract images from an Excel file and store them in S3.\n",
    "    return a dictionary mapping image names to S3 paths.\n",
    "    \"\"\"\n",
    "    pxl_doc = load_workbook(filename=io.BytesIO(excel_content))\n",
    "    all_images = {}\n",
    "    \n",
    "    for sheet_name in pxl_doc.sheetnames:\n",
    "        sheet = pxl_doc[sheet_name]\n",
    "        \n",
    "        for i, image in enumerate(sheet._images):\n",
    "            if isinstance(image, Image):\n",
    "                img_data = image.ref\n",
    "                pil_image = PILImage.open(img_data)\n",
    "\n",
    "                img_name = f\"{sheet_name}_image_{i}.png\"\n",
    "                saved_img_path = save_img_s3(s3fs, img_name, pil_image)\n",
    "                \n",
    "                all_images[img_name] = saved_img_path\n",
    "\n",
    "    return all_images\n",
    "\n",
    "xlsx_filepath = \"DimOrder.xlsx\"\n",
    "\n",
    "\n",
    "with open(xlsx_filepath, mode=\"rb\") as f:\n",
    "    content = f.read()\n",
    "    images = extract_excel_images(s3fs, content)\n",
    "    print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aioboto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "def parse_minio_url(s3_url: str):\n",
    "    \"\"\"Parse MinIO-style S3 URL into bucket name and key.\"\"\"\n",
    "    parts = s3_url.split(\"/\", 1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(\"Invalid MinIO URL format. Expected 'BUCKET/KEY'.\")\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "async def safe_read_s3_file(s3_url: str) -> bytes:\n",
    "    \"\"\"Read a file from MinIO using aioboto3.\"\"\"\n",
    "  \n",
    "    bucket_name, key = parse_minio_url(s3_url)\n",
    "\n",
    " \n",
    "    print(f\"Bucket Name: {bucket_name}\")\n",
    "    print(f\"Key: {key}\")\n",
    "\n",
    " \n",
    "    session = aioboto3.Session()\n",
    "    async with session.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=ENDPOINT_URL,\n",
    "        aws_access_key_id=MINIO_ROOT_USER,\n",
    "        aws_secret_access_key=MINIO_ROOT_PASSWORD,\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    ) as s3_client:\n",
    "    \n",
    "        response = await s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        async with response[\"Body\"] as stream:\n",
    "            content = await stream.read()\n",
    "    return content\n",
    "\n",
    " \n",
    "s3_url = \"swparse/09a0a4ce-2545-4dda-ad45-a335db65129b_4d778f25f407e0f1a41a0e39a55f896d.md\"\n",
    "content = await safe_read_s3_file(s3_url)\n",
    "print(content.decode(\"utf-8\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
