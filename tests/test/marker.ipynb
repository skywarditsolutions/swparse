{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'settings' from 'marker.settings' (/home/heinmin2maw/Desktop/swparse/.venv/lib/python3.12/site-packages/marker/settings.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# from .utils import format_timestamp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# New changes\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfConverter\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model_dict\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_from_rendered\n",
      "File \u001b[0;32m~/Desktop/swparse/.venv/lib/python3.12/site-packages/marker/converters/pdf.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Type\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentBuilder\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayoutBuilder\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mocr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OcrBuilder\n",
      "File \u001b[0;32m~/Desktop/swparse/.venv/lib/python3.12/site-packages/marker/builders/document.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseBuilder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayoutBuilder\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'settings' from 'marker.settings' (/home/heinmin2maw/Desktop/swparse/.venv/lib/python3.12/site-packages/marker/settings.py)"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from typing import Any\n",
    " \n",
    "# import pypdfium2 as pdfium  \n",
    "import structlog\n",
    "from PIL import Image\n",
    "# from .utils import format_timestamp\n",
    "\n",
    "# New changes\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "from marker.renderers.markdown import MarkdownOutput\n",
    "from marker.schema import BlockTypes\n",
    "\n",
    "import json\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = (\n",
    "    \"1\"  # For some reason, transformers decided to use .isin for a simple op, which is not supported on MPS\n",
    ")\n",
    "model_dict: dict[str,Any] | None = None\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Filter torch pytree user warnings\n",
    "\n",
    "\n",
    " \n",
    "# tuple[str, dict[str, Image.Image], dict, list]:\n",
    "if model_dict is None:\n",
    "    logger.info(\"Loading Models\")\n",
    "    model_dict = create_model_dict()\n",
    "    logger.info(len(list(model_dict.keys())))\n",
    "\n",
    "\n",
    "with open(\"./test1.pdf\", mode=\"rb\") as f:\n",
    "    content = f.read()\n",
    "with tempfile.NamedTemporaryFile(suffix=\".pdf\") as temp_pdf:\n",
    "    temp_pdf.write(content)\n",
    "    temp_pdf.seek(0)\n",
    "    filename = temp_pdf.name\n",
    "\n",
    "    # full_text, images, out_meta, json_result = convert_single_pdf(\n",
    "    #     filename,\n",
    "    #     model_lst=model_lst,\n",
    "    #     langs=langs,\n",
    "    #     start_page=start_page,\n",
    "    #     max_pages=max_pages,\n",
    "    #     ocr_all_pages=ocr_all_pages,\n",
    "    # )\n",
    "\n",
    "    pdf_converter = PdfConverter(\n",
    "                config=None,\n",
    "                artifact_dict=model_dict,\n",
    "                processor_list=None,\n",
    "                renderer=None\n",
    "            )\n",
    "    rendered:MarkdownOutput = pdf_converter(filename)\n",
    "    print(\"images\")\n",
    "    print(rendered.images)\n",
    "\n",
    "    text, ext, images = text_from_rendered(rendered)\n",
    "    with open(\"./pdf/new_out.json\", mode=\"w\") as f:\n",
    "        f.write(json.dumps(rendered.metadata, indent=2))\n",
    "    # save_output(rendered, \"pdf\", \"output.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "# def convert_single_pdf(\n",
    "#     fname: str,\n",
    "#     model_lst: list,\n",
    "#     max_pages: int | None = None,\n",
    "#     start_page: int | None = None,\n",
    "#     metadata: dict | None = None,\n",
    "#     langs: list[str] | None = None,\n",
    "#     batch_multiplier: int = 1,\n",
    "#     ocr_all_pages: bool = False,\n",
    "# ) -> tuple[str, dict[str, Image.Image], dict, list]:\n",
    "#     ocr_all_pages = ocr_all_pages or settings.OCR_ALL_PAGES\n",
    "\n",
    "#     if metadata:\n",
    "#         langs = metadata.get(\"languages\", langs)\n",
    "\n",
    "#     langs = replace_langs_with_codes(langs)\n",
    "#     validate_langs(langs)\n",
    "\n",
    "#     # Find the filetype\n",
    "#     filetype = find_filetype(fname)\n",
    "\n",
    "#     # Setup output metadata\n",
    "#     out_meta = {\n",
    "#         \"languages\": langs,\n",
    "#         \"filetype\": filetype,\n",
    "#         \"pages_metadata\": [],\n",
    "#     }\n",
    "\n",
    "#     if filetype == \"other\":  # We can't process this file\n",
    "#         return \"\", {}, out_meta\n",
    "\n",
    "#     # Get initial text blocks from the pdf\n",
    "#     doc = pdfium.PdfDocument(fname)\n",
    "#     pages, toc = get_text_blocks(\n",
    "#         doc,\n",
    "#         fname,\n",
    "#         max_pages=max_pages,\n",
    "#         start_page=start_page,\n",
    "#     )\n",
    "#     out_meta.update(\n",
    "#         {\n",
    "#             \"pdf_toc\": toc,\n",
    "#             \"pages\": len(pages),\n",
    "#         },\n",
    "#     )\n",
    "\n",
    "#     # Trim pages from doc to align with start page\n",
    "#     if start_page:\n",
    "#         for page_idx in range(start_page):\n",
    "#             doc.del_page(0)\n",
    "\n",
    "#     # Unpack models from list\n",
    "#     texify_model, layout_model, order_model, detection_model, ocr_model, table_rec_model = model_lst\n",
    "#     # Identify text lines on pages\n",
    "#     surya_detection_start = time.time()\n",
    "#     surya_detection(doc, pages, detection_model, batch_multiplier=batch_multiplier)\n",
    "#     flush_cuda_memory()\n",
    "#     surya_detection_end = time.time()\n",
    "\n",
    "#     OCR_start = time.time()\n",
    "#     # OCR pages as needed\n",
    "#     pages, ocr_stats = run_ocr(\n",
    "#         doc,\n",
    "#         pages,\n",
    "#         langs,\n",
    "#         ocr_model,\n",
    "#         batch_multiplier=batch_multiplier,\n",
    "#         ocr_all_pages=ocr_all_pages,\n",
    "#     )\n",
    "#     flush_cuda_memory()\n",
    "#     OCR_end = time.time()\n",
    "\n",
    "#     out_meta[\"ocr_stats\"] = ocr_stats\n",
    "#     if len([b for p in pages for b in p.blocks]) == 0:\n",
    "#         logger.info(f\"Could not extract any text blocks for {fname}\")\n",
    "#         return \"\", {}, out_meta\n",
    "\n",
    "#     surya_laytout_start = time.time()\n",
    "#     surya_layout(doc, pages, layout_model, batch_multiplier=batch_multiplier)\n",
    "#     flush_cuda_memory()\n",
    "#     surya_laytout_end = time.time()\n",
    "\n",
    "#     # Find headers and footers\n",
    "#     header_footer_start = time.time()\n",
    "#     bad_span_ids = filter_header_footer(pages)\n",
    "#     out_meta[\"block_stats\"] = {\"header_footer\": len(bad_span_ids)}\n",
    "#     header_footer_end = time.time()\n",
    "\n",
    "#     # Add block types in\n",
    "#     annotate_block_types(pages)\n",
    "\n",
    "#     # Find reading order for blocks\n",
    "#     # Sort blocks by reading order\n",
    "#     surya_reading_order_start = time.time()\n",
    "#     surya_order(doc, pages, order_model, batch_multiplier=batch_multiplier)\n",
    "#     sort_blocks_in_reading_order(pages)\n",
    "#     flush_cuda_memory()\n",
    "#     surya_reading_order_end = time.time()\n",
    "\n",
    "#     # Dump debug data if flags are set\n",
    "#     dump_debug_start = time.time()\n",
    "#     draw_page_debug_images(fname, pages)\n",
    "#     dump_bbox_debug_data(fname, pages)\n",
    "#     dump_debug_end = time.time()\n",
    "\n",
    "#     # Fix code blocks\n",
    "#     identify_code_start = time.time()\n",
    "#     code_block_count = identify_code_blocks(pages)\n",
    "#     out_meta[\"block_stats\"][\"code\"] = code_block_count\n",
    "#     indent_blocks(pages)\n",
    "#     identify_code_end = time.time()\n",
    "\n",
    "#     # Fix table blocks\n",
    "#     table_block_start = time.time()\n",
    "#     table_count = format_tables(pages, doc, fname, detection_model, table_rec_model, ocr_model)\n",
    "#     table_block_end = time.time()\n",
    "#     out_meta[\"block_stats\"][\"table\"] = table_count\n",
    "\n",
    "#     for page in pages:\n",
    "#         for block in page.blocks:\n",
    "#             block.filter_spans(bad_span_ids)\n",
    "#             block.filter_bad_span_types()\n",
    "\n",
    "#     equation_start = time.time()\n",
    "#     filtered, eq_stats = replace_equations(\n",
    "#         doc,\n",
    "#         pages,\n",
    "#         texify_model,\n",
    "#         batch_multiplier=batch_multiplier,\n",
    "#     )\n",
    "#     flush_cuda_memory()\n",
    "#     equation_end = time.time()\n",
    "#     out_meta[\"block_stats\"][\"equations\"] = eq_stats\n",
    "\n",
    "#     # Extract images and figures\n",
    "#     extract_images_start = time.time()\n",
    "#     if settings.EXTRACT_IMAGES:\n",
    "#         extract_images(doc, pages)\n",
    "\n",
    "#     extract_images_end = time.time()\n",
    "#     # Split out headers\n",
    "#     split_heading_blocks(pages)\n",
    "#     infer_heading_levels(pages)\n",
    "#     find_bold_italic(pages)\n",
    "\n",
    "#     # Use headers to compute a table of contents\n",
    "#     out_meta[\"computed_toc\"] = compute_toc(pages)\n",
    "\n",
    "#     # Copy to avoid changing original data\n",
    "#     merging_sections_start = time.time()\n",
    "#     merged_lines = merge_spans(filtered)\n",
    "#     text_blocks = merge_lines(merged_lines)\n",
    "#     text_blocks = filter_common_titles(text_blocks)\n",
    "#     full_text = get_full_text(text_blocks)\n",
    "#     merging_sections_end = time.time()\n",
    "\n",
    "#     # Handle empty blocks being joined\n",
    "#     full_text = cleanup_text(full_text)\n",
    "\n",
    "#     # Replace bullet characters with a -\n",
    "#     full_text = replace_bullets(full_text)\n",
    "\n",
    "#     doc_images = images_to_dict(pages)\n",
    "\n",
    "#     json_result = []\n",
    "#     per_page_start = time.time()\n",
    "#     for page_idx, page in enumerate(filtered):\n",
    "\n",
    "#         page_text = get_page_text(page)\n",
    "#         page_merged_lines = merge_spans([page])\n",
    "#         page_text_blocks = merge_lines(page_merged_lines)\n",
    "#         page_text_blocks = filter_common_titles(page_text_blocks)\n",
    "#         page_md = get_full_text(page_text_blocks)\n",
    "\n",
    "#         page_md = cleanup_text(page_md)\n",
    "\n",
    "#         page_md = replace_bullets(page_md)\n",
    "\n",
    "#         doc_images = images_to_dict([page])\n",
    "\n",
    "#         page_metadata = {\n",
    "#                 \"page\": page_idx + 1,\n",
    "#                 \"text\": page_text,\n",
    "#                 \"md\":page_md,\n",
    "#                 \"doc_images\":doc_images,\n",
    "#                 \"status\": \"OK\",\n",
    "#                 \"links\": [],\n",
    "#                 \"width\": page.width,\n",
    "#                 \"height\": page.height,\n",
    "#                 \"triggeredAutoMode\": False,\n",
    "#         }\n",
    "#         json_result.append(page_metadata)\n",
    "#     per_page_end = time.time()\n",
    "\n",
    "#     logger.info(f\"surya text line detection start: {format_timestamp(surya_detection_start)}\")\n",
    "#     logger.info(f\"surya text line detection end: {format_timestamp(surya_detection_end)}\")\n",
    "#     logger.info(f\"surya text line detection time taken {format_timestamp(surya_detection_end - surya_detection_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"OCR start: {format_timestamp(OCR_start)}\")\n",
    "#     logger.info(f\"OCR end: {format_timestamp(OCR_end)}\")\n",
    "#     logger.info(f\"OCR time taken {format_timestamp(OCR_end - OCR_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"surya_layout start: {format_timestamp(surya_laytout_start)}\")\n",
    "#     logger.info(f\"surya_layout end: {format_timestamp(surya_laytout_end)}\")\n",
    "#     logger.info(f\"surya_layout time taken {format_timestamp(surya_laytout_end - surya_laytout_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"header_footer start: {format_timestamp(header_footer_start)}\")\n",
    "#     logger.info(f\"header_footer end: {format_timestamp(header_footer_end)}\")\n",
    "#     logger.info(f\"header_footer time taken {format_timestamp(header_footer_end - header_footer_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"surya_reading_order start: {format_timestamp(surya_reading_order_start)}\")\n",
    "#     logger.info(f\"surya_reading_order end: {format_timestamp(surya_reading_order_end)}\")\n",
    "#     logger.info(f\"surya_reading_order time taken {format_timestamp(surya_reading_order_end - surya_reading_order_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"dump_debug start: {format_timestamp(dump_debug_start)}\")\n",
    "#     logger.info(f\"dump_debug end: {format_timestamp(dump_debug_end)}\")\n",
    "#     logger.info(f\"dump_debug time taken {format_timestamp(dump_debug_end - dump_debug_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"identify_code start: {format_timestamp(identify_code_start)}\")\n",
    "#     logger.info(f\"identify_code end: {format_timestamp(identify_code_end)}\")\n",
    "#     logger.info(f\"identify_code time taken {format_timestamp(identify_code_end - identify_code_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"table_block start: {format_timestamp(table_block_start)}\")\n",
    "#     logger.info(f\"table_block end: {format_timestamp(table_block_end)}\")\n",
    "#     logger.info(f\"table_block time taken {format_timestamp(table_block_end - table_block_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"equation start: {format_timestamp(equation_start)}\")\n",
    "#     logger.info(f\"equation end: {format_timestamp(equation_end)}\")\n",
    "#     logger.info(f\"equation time taken {format_timestamp(equation_end - equation_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"extract_images start: {format_timestamp(extract_images_start)}\")\n",
    "#     logger.info(f\"extract_images end: {format_timestamp(extract_images_end)}\")\n",
    "#     logger.info(f\"extract_images time taken {format_timestamp(extract_images_end - extract_images_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"merging_sections start: {format_timestamp(merging_sections_start)}\")\n",
    "#     logger.info(f\"merging_sections end: {format_timestamp(merging_sections_end)}\")\n",
    "#     logger.info(f\"merging_sections time taken {format_timestamp(merging_sections_end - merging_sections_start)}\\n\\n\")\n",
    "\n",
    "#     logger.info(f\"per_page start: {format_timestamp(per_page_start)}\")\n",
    "#     logger.info(f\"per_page end: {format_timestamp(per_page_end)}\")\n",
    "#     logger.info(f\"per_page time taken {format_timestamp(per_page_end - per_page_start)}\\n\\n\")\n",
    "\n",
    "#     return full_text, doc_images, out_meta, json_result\n",
    "\n",
    "\n",
    "\n",
    "# def get_page_text(page: Page) -> str:\n",
    "#     page_text = []\n",
    "\n",
    "#     for block in page.blocks:\n",
    "\n",
    "#         for line in block.lines:\n",
    "\n",
    "#             for span in line.spans:\n",
    "#                 if span.text.strip():\n",
    "#                     page_text.append(span.text.strip())  \n",
    "\n",
    "#     return \"\\n\".join(page_text)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
