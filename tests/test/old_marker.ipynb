{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_det3 on device cpu with dtype torch.float32\n",
      "Loaded detection model vikp/surya_layout3 on device cpu with dtype torch.float32\n",
      "Loaded reading order model vikp/surya_order on device cpu with dtype torch.float32\n",
      "Loaded recognition model vikp/surya_rec2 on device cpu with dtype torch.float32\n",
      "Loaded texify model to cpu with torch.float32 dtype\n",
      "Loaded recognition model vikp/surya_tablerec on device cpu with dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "from marker.models import load_all_models\n",
    "import typing\n",
    "\n",
    "model_lst: list[typing.Any] = []\n",
    "\n",
    "model_lst.extend(load_all_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import pypdfium2 as pdfium  # Needs to be at the top to avoid warnings\n",
    "\n",
    "from marker.cleaners.bullets import replace_bullets\n",
    "from marker.cleaners.code import identify_code_blocks, indent_blocks\n",
    "from marker.cleaners.fontstyle import find_bold_italic\n",
    "from marker.cleaners.headers import filter_common_titles, filter_header_footer\n",
    "from marker.cleaners.headings import infer_heading_levels, split_heading_blocks\n",
    "from marker.cleaners.text import cleanup_text\n",
    "from marker.cleaners.toc import compute_toc\n",
    "from marker.debug.data import draw_page_debug_images, dump_bbox_debug_data\n",
    "from marker.equations.equations import replace_equations\n",
    "from marker.images.extract import extract_images\n",
    "from marker.images.save import images_to_dict\n",
    "from marker.layout.layout import annotate_block_types, surya_layout\n",
    "from marker.layout.order import sort_blocks_in_reading_order, surya_order\n",
    "from marker.ocr.detection import surya_detection\n",
    "from marker.ocr.lang import replace_langs_with_codes, validate_langs\n",
    "from marker.ocr.recognition import run_ocr\n",
    "from marker.pdf.extract_text import get_text_blocks\n",
    "from marker.pdf.utils import find_filetype\n",
    "from marker.postprocessors.markdown import get_full_text, merge_lines, merge_spans\n",
    "from marker.schema.page import Page\n",
    "from marker.settings import settings\n",
    "from marker.tables.table import format_tables\n",
    "from marker.utils import flush_cuda_memory\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "# utilities \n",
    "from typing import Any\n",
    "import json\n",
    "import os\n",
    "\n",
    "def format_timestamp(timestamp:float) ->str:\n",
    "    value = datetime.fromtimestamp(timestamp)\n",
    "    return value.strftime('%S:') + f\"{int(value.strftime('%f')) // 1000}\"\n",
    "\n",
    "def convert_single_pdf(\n",
    "    fname: str,\n",
    "    model_lst: list,\n",
    "    max_pages: int | None = None,\n",
    "    start_page: int | None = None,\n",
    "    metadata: dict | None = None,\n",
    "    langs: list[str] | None = None,\n",
    "    batch_multiplier: int = 1,\n",
    "    ocr_all_pages: bool = False,\n",
    ") -> tuple[str, dict[str, Image.Image], dict, list]:\n",
    "    ocr_all_pages = ocr_all_pages or settings.OCR_ALL_PAGES\n",
    "\n",
    "    if metadata:\n",
    "        langs = metadata.get(\"languages\", langs)\n",
    "\n",
    "    langs = replace_langs_with_codes(langs)\n",
    "    validate_langs(langs)\n",
    "\n",
    "    # Find the filetype\n",
    "    filetype = find_filetype(fname)\n",
    "\n",
    "    # Setup output metadata\n",
    "    out_meta = {\n",
    "        \"languages\": langs,\n",
    "        \"filetype\": filetype,\n",
    "        \"pages_metadata\": [],\n",
    "    }\n",
    "\n",
    "    if filetype == \"other\":  # We can't process this file\n",
    "        return \"\", {}, out_meta\n",
    "\n",
    "    # Get initial text blocks from the pdf\n",
    "    doc = pdfium.PdfDocument(fname)\n",
    "    pages, toc = get_text_blocks(\n",
    "        doc,\n",
    "        fname,\n",
    "        max_pages=max_pages,\n",
    "        start_page=start_page,\n",
    "    )\n",
    "    out_meta.update(\n",
    "        {\n",
    "            \"pdf_toc\": toc,\n",
    "            \"pages\": len(pages),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Trim pages from doc to align with start page\n",
    "    if start_page:\n",
    "        for page_idx in range(start_page):\n",
    "            doc.del_page(0)\n",
    "\n",
    "    # Unpack models from list\n",
    "    texify_model, layout_model, order_model, detection_model, ocr_model, table_rec_model = model_lst\n",
    "    # Identify text lines on pages\n",
    "    surya_detection_start = time.time()\n",
    "    surya_detection(doc, pages, detection_model, batch_multiplier=batch_multiplier)\n",
    "    flush_cuda_memory()\n",
    "    surya_detection_end = time.time()\n",
    "\n",
    "    OCR_start = time.time()\n",
    "    # OCR pages as needed\n",
    "    pages, ocr_stats = run_ocr(\n",
    "        doc,\n",
    "        pages,\n",
    "        langs,\n",
    "        ocr_model,\n",
    "        batch_multiplier=batch_multiplier,\n",
    "        ocr_all_pages=ocr_all_pages,\n",
    "    )\n",
    "    flush_cuda_memory()\n",
    "    OCR_end = time.time()\n",
    "\n",
    "    out_meta[\"ocr_stats\"] = ocr_stats\n",
    "    if len([b for p in pages for b in p.blocks]) == 0:\n",
    "        print(f\"Could not extract any text blocks for {fname}\")\n",
    "        return \"\", {}, out_meta\n",
    "\n",
    "    surya_laytout_start = time.time()\n",
    "    surya_layout(doc, pages, layout_model, batch_multiplier=batch_multiplier)\n",
    "    flush_cuda_memory()\n",
    "    surya_laytout_end = time.time()\n",
    "\n",
    "    # Find headers and footers\n",
    "    header_footer_start = time.time()\n",
    "    bad_span_ids = filter_header_footer(pages)\n",
    "    out_meta[\"block_stats\"] = {\"header_footer\": len(bad_span_ids)}\n",
    "    header_footer_end = time.time()\n",
    "\n",
    "    # Add block types in\n",
    "    annotate_block_types(pages)\n",
    "\n",
    "    # Find reading order for blocks\n",
    "    # Sort blocks by reading order\n",
    "    surya_reading_order_start = time.time()\n",
    "    surya_order(doc, pages, order_model, batch_multiplier=batch_multiplier)\n",
    "    sort_blocks_in_reading_order(pages)\n",
    "    flush_cuda_memory()\n",
    "    surya_reading_order_end = time.time()\n",
    "\n",
    "    # Dump debug data if flags are set\n",
    "    dump_debug_start = time.time()\n",
    "    draw_page_debug_images(fname, pages)\n",
    "    dump_bbox_debug_data(fname, pages)\n",
    "    dump_debug_end = time.time()\n",
    "\n",
    "    # Fix code blocks\n",
    "    identify_code_start = time.time()\n",
    "    code_block_count = identify_code_blocks(pages)\n",
    "    out_meta[\"block_stats\"][\"code\"] = code_block_count\n",
    "    indent_blocks(pages)\n",
    "    identify_code_end = time.time()\n",
    "\n",
    " \n",
    "    table_count = format_tables(pages, doc, fname, detection_model, table_rec_model, ocr_model)\n",
    " \n",
    "    out_meta[\"block_stats\"][\"table\"] = table_count\n",
    "\n",
    "    for page in pages:\n",
    "        for block in page.blocks:\n",
    "            block.filter_spans(bad_span_ids)\n",
    "            block.filter_bad_span_types()\n",
    "\n",
    " \n",
    "    filtered, eq_stats = replace_equations(\n",
    "        doc,\n",
    "        pages,\n",
    "        texify_model,\n",
    "        batch_multiplier=batch_multiplier,\n",
    "    )\n",
    "    flush_cuda_memory()\n",
    " \n",
    "    out_meta[\"block_stats\"][\"equations\"] = eq_stats\n",
    "\n",
    " \n",
    "    if settings.EXTRACT_IMAGES:\n",
    "        extract_images(doc, pages)\n",
    "\n",
    " \n",
    "    # Split out headers\n",
    "    split_heading_blocks(pages)\n",
    "    infer_heading_levels(pages)\n",
    "    find_bold_italic(pages)\n",
    "\n",
    "    # Use headers to compute a table of contents\n",
    "    out_meta[\"computed_toc\"] = compute_toc(pages)\n",
    "\n",
    " \n",
    "    merged_lines = merge_spans(filtered)\n",
    "    text_blocks = merge_lines(merged_lines)\n",
    "    text_blocks = filter_common_titles(text_blocks)\n",
    "    full_text = get_full_text(text_blocks)\n",
    " \n",
    "\n",
    "    # Handle empty blocks being joined\n",
    "    full_text = cleanup_text(full_text)\n",
    "\n",
    "    # Replace bullet characters with a -\n",
    "    full_text = replace_bullets(full_text)\n",
    "\n",
    "    doc_images = images_to_dict(pages)\n",
    "\n",
    "    json_result = []\n",
    " \n",
    "    for page_idx, page in enumerate(filtered):\n",
    "\n",
    "        page_text = get_page_text(page)\n",
    "        page_merged_lines = merge_spans([page])\n",
    "        page_text_blocks = merge_lines(page_merged_lines)\n",
    "        page_text_blocks = filter_common_titles(page_text_blocks)\n",
    "        page_md = get_full_text(page_text_blocks)\n",
    "\n",
    "        page_md = cleanup_text(page_md)\n",
    "\n",
    "        page_md = replace_bullets(page_md)\n",
    "\n",
    "        doc_images = images_to_dict([page])\n",
    "        images = save_images(doc_images)\n",
    "        page_metadata = {\n",
    "                \"page\": page_idx + 1,\n",
    "                \"text\": page_text,\n",
    "                \"md\":page_md,\n",
    "                \"images\":images,\n",
    "                \"status\": \"OK\",\n",
    "                \"links\": [],\n",
    "                \"width\": page.width,\n",
    "                \"height\": page.height,\n",
    "                \"triggeredAutoMode\": False,\n",
    "        }\n",
    "        json_result.append(page_metadata)\n",
    " \n",
    "\n",
    "    return full_text, doc_images, out_meta, json_result\n",
    "\n",
    "\n",
    "\n",
    "def get_page_text(page: Page) -> str:\n",
    "    page_text = []\n",
    "\n",
    "    for block in page.blocks:\n",
    "\n",
    "        for line in block.lines:\n",
    "\n",
    "            for span in line.spans:\n",
    "                if span.text.strip():\n",
    "                    page_text.append(span.text.strip())  \n",
    "\n",
    "    return \"\\n\".join(page_text)\n",
    "\n",
    "\n",
    "async def convert_xlsx_csv(\n",
    "    input: bytes\n",
    ") -> str:\n",
    "    try:\n",
    "        xlsx_data = io.BytesIO(input)\n",
    "        df = pd.read_excel(xlsx_data, header=None)\n",
    "        return df.to_csv(index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"info converting XLSX to CSV: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    " \n",
    "def save_md_file(file_name:str, content:str):\n",
    "     with open(f\"./md/{file_name}.md\", mode=\"w\") as f:\n",
    "        f.write(content)\n",
    " \n",
    "def save_str_file(file_name:str, content:str):\n",
    "     with open(f\"./txt/{file_name}\", mode=\"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "def save_json_file(file_name:str, content:Any):\n",
    "     \n",
    "    with open(f\"./json/{file_name}.json\", \"w\") as f:\n",
    "        f.write(json.dumps(content, indent=4)) \n",
    "\n",
    "\n",
    "def save_html_file(file_name:str, content:Any):\n",
    "     \n",
    "    with open(f\"./html/{file_name}.html\", \"w\") as f:\n",
    "        f.write(json.dumps(content, indent=4)) \n",
    "\n",
    "def save_images(images: dict[str, Any]) -> dict[str, str]:\n",
    "\n",
    "    output_dir = \"./old_marker_img\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    " \n",
    "    saved_paths = {}\n",
    "\n",
    "    for img_name, img in images.items():\n",
    "        file_path = os.path.join(output_dir, img_name)\n",
    "        img.save(file_path, \"PNG\", optimize=False, compress_level=3)\n",
    "        saved_paths[img_name] = file_path\n",
    "\n",
    "    return saved_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models \n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 3/3 [00:43<00:00, 14.40s/it]\n",
      "Detecting bboxes: 100%|██████████| 2/2 [00:36<00:00, 18.28s/it]\n",
      "Finding reading order: 100%|██████████| 2/2 [00:54<00:00, 27.07s/it]\n",
      "Recognizing tables: 100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "from marker.models import load_all_models\n",
    "\n",
    " \n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = (\n",
    "    \"1\"  # For some reason, transformers decided to use .isin for a simple op, which is not supported on MPS\n",
    ")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Filter torch pytree user warnings\n",
    "print(\"Models \") \n",
    "print(len(model_lst))\n",
    "if len(model_lst) == 0:\n",
    "    print(\"Loading Models\")\n",
    "    model_lst.extend(load_all_models())\n",
    "    print(len(model_lst))\n",
    "\n",
    "with open(\"./test1.pdf\", mode=\"rb\") as f:\n",
    "    content = f.read()\n",
    "with tempfile.NamedTemporaryFile(suffix=\".pdf\") as temp_pdf:\n",
    "    temp_pdf.write(content)\n",
    "    temp_pdf.seek(0)\n",
    "    filename = temp_pdf.name\n",
    "\n",
    "    full_text, images, out_meta, json_result = convert_single_pdf(\n",
    "        filename,\n",
    "        model_lst=model_lst,\n",
    "        langs=None,\n",
    "        start_page=None,\n",
    "        max_pages=None,\n",
    "        ocr_all_pages=False,\n",
    "    )\n",
    " \n",
    "    save_json_file(\"old_json\", json_result)\n",
    " \n",
    " #2m 26.5s\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
